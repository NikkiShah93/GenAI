{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9245324f-0c45-436d-8113-5f8638226bb2",
   "metadata": {},
   "source": [
    "### LLM Fundamentals\n",
    "\n",
    "In this notebook we will go through the fundamentals of the **LLM**.\n",
    "\n",
    "The steps are as follows:\n",
    "- Load the data\n",
    "- Encode the data\n",
    "- Converting our text to tensors\n",
    "- Train/test split\n",
    "- Staring with Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c955044e-1ba4-4123-be75-add835cf19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first the imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fec8c-5aeb-42b0-9866-877d111165d6",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4d68be-4c84-47c3-b896-b106648091c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = Path(DATA_PATH)\n",
    "FILE_NAME = 'wizardOfOz.txt'\n",
    "FILE_PATH = PATH / FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60db8bc7-ab79-4e4e-ba55-08b437d8c14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿ Dorothy and the Wizard in Oz\n",
      "\n",
      "\n",
      "  A Faithful Record of Their Amazing Adventures\n",
      "    in an Undergrou\n"
     ]
    }
   ],
   "source": [
    "## opening the file\n",
    "with open(FILE_PATH, 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "## checking the first 100 char\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb01b39-49f2-4a28-8e88-0f62a2f10c80",
   "metadata": {},
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5e9358-2432-4c91-b756-cddf520b73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "## first we want to create a set of char\n",
    "chars = sorted(set(text))\n",
    "## checking how many distinct char are in the text\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "## and then assign a number to each char\n",
    "int_to_string = {i:l for i, l in enumerate(chars)}\n",
    "string_to_int = {l:i for i, l in enumerate(chars)}\n",
    "## and then move on to creating our encoding and decoding functions\n",
    "encode = lambda l:[string_to_int[x] for x in l]\n",
    "decode = lambda i:''.join([int_to_string[x] for x in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ba0b36-3075-4cc8-8638-9a7d8f7664e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 63, 66, 63, 68, 56, 73]\n",
      "Dorothy\n"
     ]
    }
   ],
   "source": [
    "## we can test our encoder and decoder now\n",
    "print(encode('Dorothy'))\n",
    "print(decode(encode('Dorothy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f55bf-9885-4359-9878-2f7ee6a1c922",
   "metadata": {},
   "source": [
    "### Converting our text to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a241ad82-eb62-4792-965b-c69df279837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([230550]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([75,  1, 27, 63, 66, 63, 68, 56, 73,  1, 49, 62, 52,  1, 68, 56, 53,  1,\n",
       "        46, 57, 74, 49, 66, 52,  1, 57, 62,  1, 38, 74,  0,  0,  0,  1,  1, 24,\n",
       "         1, 29, 49, 57, 68, 56, 54, 69, 60,  1, 41, 53, 51, 63, 66, 52,  1, 63,\n",
       "        54,  1, 43, 56, 53, 57, 66,  1, 24, 61, 49, 74, 57, 62, 55,  1, 24, 52,\n",
       "        70, 53, 62, 68, 69, 66, 53, 67,  0,  1,  1,  1,  1, 57, 62,  1, 49, 62,\n",
       "         1, 44, 62, 52, 53, 66, 55, 66, 63, 69])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tensor = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(text_tensor.size(), type(text_tensor))\n",
    "text_tensor[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50ecc10d-c1dc-452b-984f-5e8c6a238651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([184440]), torch.Size([46110]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we're simply splitting the data into train and test sets\n",
    "train_data, test_data = np.split(text_tensor, [int(.8*len(text_tensor))])\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f6ccaa-98e0-4e8a-8592-92c633bf5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the context is tensor([75]) the target will be 1\n",
      "When the context is tensor([75,  1]) the target will be 27\n",
      "When the context is tensor([75,  1, 27]) the target will be 63\n",
      "When the context is tensor([75,  1, 27, 63]) the target will be 66\n",
      "When the context is tensor([75,  1, 27, 63, 66]) the target will be 63\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63]) the target will be 68\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63, 68]) the target will be 56\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63, 68, 56]) the target will be 73\n"
     ]
    }
   ],
   "source": [
    "## next we have to define a block size for our model\n",
    "block_size = 8\n",
    "## this means, the model will look at 8 sequences\n",
    "## at each round of training\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f'When the context is {context} the target will be {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90440876-1737-4dbd-93b5-bf6ad6829ca1",
   "metadata": {},
   "source": [
    "The reason we're looping through the `block_size` range, is to have our model get used to seeing anything from `1` to the `block_size` length of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "525eac3b-c93c-4c28-8bff-9de027f65fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "## we also need to break our data into batches for faster computations\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    random_inx = torch.randint(high=len(data)-block_size, size = (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in random_inx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_inx])\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = get_batch('train')\n",
    "X_test, y_test = get_batch('test')\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3d88b3a-8b1d-42dc-ba4a-c7ffc23bdddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: When context is tensor([49]) the target is 68\n",
      "Batch 0: When context is tensor([49, 68]) the target is 0\n",
      "Batch 0: When context is tensor([49, 68,  0]) the target is 56\n",
      "Batch 0: When context is tensor([49, 68,  0, 56]) the target is 57\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57]) the target is 67\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57, 67]) the target is 1\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57, 67,  1]) the target is 54\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57, 67,  1, 54]) the target is 53\n",
      "Batch 1: When context is tensor([53]) the target is 1\n",
      "Batch 1: When context is tensor([53,  1]) the target is 67\n",
      "Batch 1: When context is tensor([53,  1, 67]) the target is 69\n",
      "Batch 1: When context is tensor([53,  1, 67, 69]) the target is 52\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52]) the target is 52\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52, 52]) the target is 53\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52, 52, 53]) the target is 62\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52, 52, 53, 62]) the target is 60\n",
      "Batch 2: When context is tensor([0]) the target is 51\n",
      "Batch 2: When context is tensor([ 0, 51]) the target is 63\n",
      "Batch 2: When context is tensor([ 0, 51, 63]) the target is 70\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70]) the target is 53\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53]) the target is 66\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53, 66]) the target is 53\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53, 66, 53]) the target is 52\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53, 66, 53, 52]) the target is 1\n",
      "Batch 3: When context is tensor([55]) the target is 63\n",
      "Batch 3: When context is tensor([55, 63]) the target is 73\n",
      "Batch 3: When context is tensor([55, 63, 73]) the target is 60\n",
      "Batch 3: When context is tensor([55, 63, 73, 60]) the target is 53\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53]) the target is 67\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53, 67]) the target is 0\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53, 67,  0]) the target is 50\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53, 67,  0, 50]) the target is 53\n"
     ]
    }
   ],
   "source": [
    "## and we can loop through the batches in our set\n",
    "for b in range(batch_size):\n",
    "    for i in range(block_size):\n",
    "        context = X_train[b,:i+1]\n",
    "        target = y_train[b, i]\n",
    "        print(f'Batch {b}: When context is {context} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0e1ac-24dd-4e29-a84c-7a63b2c35267",
   "metadata": {},
   "source": [
    "### The Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71c8e5cb-57f6-4565-98eb-1520e60f7198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 76])\n",
      "expected loss 4.33\n",
      "claculated loss 4.76\n"
     ]
    }
   ],
   "source": [
    "## we will be inheriting from the nn.Module\n",
    "## and then use the embedding from nn to build our class\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        ## we're basically creating a wrapper\n",
    "        ## around a tensor of vocab_size x vocab_size\n",
    "        ## and each index that's passed to the model\n",
    "        ## will go and take out it's row from that table \n",
    "        self.embedding_table = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                           embedding_dim=vocab_size)\n",
    "    def forward(self, x, target=None):\n",
    "        ## pytorch will re-arrange it into (Batch, Time, Channel) tensor\n",
    "        ## where batch is the batch_size, time is the block_size\n",
    "        ## and channel is the vocab_size\n",
    "        ## so in our case will be (4, 8, 76)\n",
    "        logits = self.embedding_table(x)\n",
    "        ## we also need the loss\n",
    "        ## which we'll be using the -log(likelihood)\n",
    "        ## the issue with the functional cross entropy\n",
    "        ## is that it needs the inputs to be in (Batch*Time, Channel)\n",
    "        ## so we have to change the shape of our logits and targets\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, num_max_token):\n",
    "        for _ in range(num_max_token):\n",
    "            ## we want to get the predictions again\n",
    "            logits, loss = self(x)\n",
    "            ## and we only want the last block (Batch, Block, Vocab)\n",
    "            logits = logits[:, -1, :] ## (Batch, Vocab)\n",
    "            ## and then we apply the softmax to get the probabilities\n",
    "            probs = torch.softmax(logits, dim=-1) ## still (Batch, Vocab)\n",
    "            ## and then get a sample from the probablity distribution\n",
    "            next_inx = torch.multinomial(probs,num_samples=1) ## (B, 1)\n",
    "            ## and then append the next index to the x\n",
    "            x = torch.cat((x, next_inx), dim=1) ## (Batch, Block + 1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "output, loss = model(X_train, y_train)\n",
    "print(output.shape)\n",
    "## we're expecting the initial entropy to be\n",
    "## -ln(1/vocab_size)\n",
    "print(f'expected loss {-np.log(1/vocab_size):.2f}')\n",
    "print(f'claculated loss {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbf68e-bbbd-446a-994b-63d47de1cf3d",
   "metadata": {},
   "source": [
    "And we can see that the initial loss is higher than expected, which means the initial guesses are not completely diffused, and we have some entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "610f59bf-65e2-4c58-a34e-dd605e265dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ster toweo(o6aoygaï»¿.mI7ku(;ViEdvnhh;BeoPYP\"sbx8-yIfltBpJRG-pOqYpwkb\"O7;AYWpC:MErhh:veRORfv3y&prFeNFlBcr:NiMA\n"
     ]
    }
   ],
   "source": [
    "## checking the generate method\n",
    "## which is completely random at the moment\n",
    "## because we haven't yet trained the model\n",
    "print(decode(model.generate(X_test, num_max_token=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd155853-c35a-421a-aaab-045045d8b2ac",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1bbdfb7-5aa8-4c3f-9038-4fcc07051a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## like any other ML model\n",
    "## we need an optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d680a0b1-dc3c-40b9-9073-fa0c0c9611ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss is 4.8435\n",
      "Epoch 1000 Loss is 3.7858\n",
      "Epoch 2000 Loss is 3.2759\n",
      "Epoch 3000 Loss is 2.7724\n",
      "Epoch 4000 Loss is 2.6515\n",
      "Epoch 5000 Loss is 2.5495\n",
      "Epoch 6000 Loss is 2.4680\n",
      "Epoch 7000 Loss is 2.3957\n",
      "Epoch 8000 Loss is 2.5189\n",
      "Epoch 9000 Loss is 2.5125\n",
      "Epoch 10000 Loss is 2.4334\n",
      "Epoch 11000 Loss is 2.3051\n",
      "Epoch 12000 Loss is 2.3299\n",
      "Epoch 13000 Loss is 2.3017\n",
      "Epoch 14000 Loss is 2.4648\n",
      "Epoch 15000 Loss is 2.4087\n",
      "Epoch 16000 Loss is 2.3207\n",
      "Epoch 17000 Loss is 2.4632\n",
      "Epoch 18000 Loss is 2.4763\n",
      "Epoch 19000 Loss is 2.3227\n"
     ]
    }
   ],
   "source": [
    "## the next step is to train the model\n",
    "batch_size = 32\n",
    "epochs = 20000\n",
    "for e in range(epochs):\n",
    "    ## get the X and y\n",
    "    X_train, y_train = get_batch('train')\n",
    "    logits, loss = model(X_train, y_train)\n",
    "    ## zeroing the gradient\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ## and then backpropagation\n",
    "    loss.backward()\n",
    "    ## and then taking a step\n",
    "    optimizer.step()\n",
    "    if e%1000==0:\n",
    "        print(f'Epoch {e} Loss is {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04d9c92b-93ab-45c9-bf33-4ef4907f64c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ster toweabadmoom pngal,\"Bu f t ne I wandns n's. ppedes 3.\n",
      "\n",
      "ey engiousece\n",
      "\n",
      "rothe,\"ckithe besatourskevoly n bleant athy wncleancaros aut\n",
      "\n",
      "\n",
      "\n",
      "Thacecet thout Yon tantt, her averesogond bely stowathe ie, ce angaive. d.\n",
      "\n",
      "anly.\"At at, idins torerowadvo s mamers Zere aplor sinss I'sa, angen at tatheyous he a w t ig\n"
     ]
    }
   ],
   "source": [
    "## now let's check to see what will our model generate after training\n",
    "print(decode(model.generate(X_test, num_max_token=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec29414-0be3-4654-9eef-7d856e610468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
