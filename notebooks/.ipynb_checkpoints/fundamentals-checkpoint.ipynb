{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9245324f-0c45-436d-8113-5f8638226bb2",
   "metadata": {},
   "source": [
    "### LLM Fundamentals\n",
    "\n",
    "In this notebook we will go through the fundamentals of the **LLM**.\n",
    "\n",
    "The steps are as follows:\n",
    "- Load the data\n",
    "- Encode the data\n",
    "- Converting our text to tensors\n",
    "- Train/test split\n",
    "- Staring with Bigram model\n",
    "- Creating a self-attention unit\n",
    "- Staring with the Head class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c955044e-1ba4-4123-be75-add835cf19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first the imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fec8c-5aeb-42b0-9866-877d111165d6",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4d68be-4c84-47c3-b896-b106648091c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = Path(DATA_PATH)\n",
    "FILE_NAME = 'wizardOfOz.txt'\n",
    "FILE_PATH = PATH / FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60db8bc7-ab79-4e4e-ba55-08b437d8c14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿ Dorothy and the Wizard in Oz\n",
      "\n",
      "\n",
      "  A Faithful Record of Their Amazing Adventures\n",
      "    in an Undergrou\n"
     ]
    }
   ],
   "source": [
    "## opening the file\n",
    "with open(FILE_PATH, 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "## checking the first 100 char\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb01b39-49f2-4a28-8e88-0f62a2f10c80",
   "metadata": {},
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5e9358-2432-4c91-b756-cddf520b73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "## first we want to create a set of char\n",
    "chars = sorted(set(text))\n",
    "## checking how many distinct char are in the text\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "## and then assign a number to each char\n",
    "int_to_string = {i:l for i, l in enumerate(chars)}\n",
    "string_to_int = {l:i for i, l in enumerate(chars)}\n",
    "## and then move on to creating our encoding and decoding functions\n",
    "encode = lambda l:[string_to_int[x] for x in l]\n",
    "decode = lambda i:''.join([int_to_string[x] for x in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ba0b36-3075-4cc8-8638-9a7d8f7664e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 63, 66, 63, 68, 56, 73]\n",
      "Dorothy\n"
     ]
    }
   ],
   "source": [
    "## we can test our encoder and decoder now\n",
    "print(encode('Dorothy'))\n",
    "print(decode(encode('Dorothy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f55bf-9885-4359-9878-2f7ee6a1c922",
   "metadata": {},
   "source": [
    "### Converting our text to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a241ad82-eb62-4792-965b-c69df279837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([230550]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([75,  1, 27, 63, 66, 63, 68, 56, 73,  1, 49, 62, 52,  1, 68, 56, 53,  1,\n",
       "        46, 57, 74, 49, 66, 52,  1, 57, 62,  1, 38, 74,  0,  0,  0,  1,  1, 24,\n",
       "         1, 29, 49, 57, 68, 56, 54, 69, 60,  1, 41, 53, 51, 63, 66, 52,  1, 63,\n",
       "        54,  1, 43, 56, 53, 57, 66,  1, 24, 61, 49, 74, 57, 62, 55,  1, 24, 52,\n",
       "        70, 53, 62, 68, 69, 66, 53, 67,  0,  1,  1,  1,  1, 57, 62,  1, 49, 62,\n",
       "         1, 44, 62, 52, 53, 66, 55, 66, 63, 69])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tensor = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(text_tensor.size(), type(text_tensor))\n",
    "text_tensor[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50ecc10d-c1dc-452b-984f-5e8c6a238651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([184440]), torch.Size([46110]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we're simply splitting the data into train and test sets\n",
    "train_data, test_data = np.split(text_tensor, [int(.8*len(text_tensor))])\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f6ccaa-98e0-4e8a-8592-92c633bf5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the context is tensor([75]) the target will be 1\n",
      "When the context is tensor([75,  1]) the target will be 27\n",
      "When the context is tensor([75,  1, 27]) the target will be 63\n",
      "When the context is tensor([75,  1, 27, 63]) the target will be 66\n",
      "When the context is tensor([75,  1, 27, 63, 66]) the target will be 63\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63]) the target will be 68\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63, 68]) the target will be 56\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63, 68, 56]) the target will be 73\n"
     ]
    }
   ],
   "source": [
    "## next we have to define a block size for our model\n",
    "block_size = 8\n",
    "## this means, the model will look at 8 sequences\n",
    "## at each round of training\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f'When the context is {context} the target will be {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90440876-1737-4dbd-93b5-bf6ad6829ca1",
   "metadata": {},
   "source": [
    "The reason we're looping through the `block_size` range, is to have our model get used to seeing anything from `1` to the `block_size` length of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "525eac3b-c93c-4c28-8bff-9de027f65fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "## we also need to break our data into batches for faster computations\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    random_inx = torch.randint(high=len(data)-block_size, size = (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in random_inx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_inx])\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = get_batch('train')\n",
    "X_test, y_test = get_batch('test')\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3d88b3a-8b1d-42dc-ba4a-c7ffc23bdddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: When context is tensor([49]) the target is 68\n",
      "Batch 0: When context is tensor([49, 68]) the target is 0\n",
      "Batch 0: When context is tensor([49, 68,  0]) the target is 56\n",
      "Batch 0: When context is tensor([49, 68,  0, 56]) the target is 57\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57]) the target is 67\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57, 67]) the target is 1\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57, 67,  1]) the target is 54\n",
      "Batch 0: When context is tensor([49, 68,  0, 56, 57, 67,  1, 54]) the target is 53\n",
      "Batch 1: When context is tensor([53]) the target is 1\n",
      "Batch 1: When context is tensor([53,  1]) the target is 67\n",
      "Batch 1: When context is tensor([53,  1, 67]) the target is 69\n",
      "Batch 1: When context is tensor([53,  1, 67, 69]) the target is 52\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52]) the target is 52\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52, 52]) the target is 53\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52, 52, 53]) the target is 62\n",
      "Batch 1: When context is tensor([53,  1, 67, 69, 52, 52, 53, 62]) the target is 60\n",
      "Batch 2: When context is tensor([0]) the target is 51\n",
      "Batch 2: When context is tensor([ 0, 51]) the target is 63\n",
      "Batch 2: When context is tensor([ 0, 51, 63]) the target is 70\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70]) the target is 53\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53]) the target is 66\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53, 66]) the target is 53\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53, 66, 53]) the target is 52\n",
      "Batch 2: When context is tensor([ 0, 51, 63, 70, 53, 66, 53, 52]) the target is 1\n",
      "Batch 3: When context is tensor([55]) the target is 63\n",
      "Batch 3: When context is tensor([55, 63]) the target is 73\n",
      "Batch 3: When context is tensor([55, 63, 73]) the target is 60\n",
      "Batch 3: When context is tensor([55, 63, 73, 60]) the target is 53\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53]) the target is 67\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53, 67]) the target is 0\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53, 67,  0]) the target is 50\n",
      "Batch 3: When context is tensor([55, 63, 73, 60, 53, 67,  0, 50]) the target is 53\n"
     ]
    }
   ],
   "source": [
    "## and we can loop through the batches in our set\n",
    "for b in range(batch_size):\n",
    "    for i in range(block_size):\n",
    "        context = X_train[b,:i+1]\n",
    "        target = y_train[b, i]\n",
    "        print(f'Batch {b}: When context is {context} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0e1ac-24dd-4e29-a84c-7a63b2c35267",
   "metadata": {},
   "source": [
    "### The Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71c8e5cb-57f6-4565-98eb-1520e60f7198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 76])\n",
      "expected loss 4.33\n",
      "claculated loss 4.76\n"
     ]
    }
   ],
   "source": [
    "## we will be inheriting from the nn.Module\n",
    "## and then use the embedding from nn to build our class\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        ## we're basically creating a wrapper\n",
    "        ## around a tensor of vocab_size x vocab_size\n",
    "        ## and each index that's passed to the model\n",
    "        ## will go and take out it's row from that table \n",
    "        self.embedding_table = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                           embedding_dim=vocab_size)\n",
    "    def forward(self, x, target=None):\n",
    "        ## pytorch will re-arrange it into (Batch, Time, Channel) tensor\n",
    "        ## where batch is the batch_size, time is the block_size\n",
    "        ## and channel is the vocab_size\n",
    "        ## so in our case will be (4, 8, 76)\n",
    "        logits = self.embedding_table(x)\n",
    "        ## we also need the loss\n",
    "        ## which we'll be using the -log(likelihood)\n",
    "        ## the issue with the functional cross entropy\n",
    "        ## is that it needs the inputs to be in (Batch*Time, Channel)\n",
    "        ## so we have to change the shape of our logits and targets\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, num_max_token):\n",
    "        for _ in range(num_max_token):\n",
    "            ## we want to get the predictions again\n",
    "            logits, loss = self(x)\n",
    "            ## and we only want the last block (Batch, Block, Vocab)\n",
    "            logits = logits[:, -1, :] ## (Batch, Vocab)\n",
    "            ## and then we apply the softmax to get the probabilities\n",
    "            probs = torch.softmax(logits, dim=-1) ## still (Batch, Vocab)\n",
    "            ## and then get a sample from the probablity distribution\n",
    "            next_inx = torch.multinomial(probs,num_samples=1) ## (B, 1)\n",
    "            ## and then append the next index to the x\n",
    "            x = torch.cat((x, next_inx), dim=1) ## (Batch, Block + 1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "output, loss = model(X_train, y_train)\n",
    "print(output.shape)\n",
    "## we're expecting the initial entropy to be\n",
    "## -ln(1/vocab_size)\n",
    "print(f'expected loss {-np.log(1/vocab_size):.2f}')\n",
    "print(f'claculated loss {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbf68e-bbbd-446a-994b-63d47de1cf3d",
   "metadata": {},
   "source": [
    "And we can see that the initial loss is higher than expected, which means the initial guesses are not completely diffused, and we have some entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "610f59bf-65e2-4c58-a34e-dd605e265dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ster toweo(o6aoygaï»¿.mI7ku(;ViEdvnhh;BeoPYP\"sbx8-yIfltBpJRG-pOqYpwkb\"O7;AYWpC:MErhh:veRORfv3y&prFeNFlBcr:NiMA\n"
     ]
    }
   ],
   "source": [
    "## checking the generate method\n",
    "## which is completely random at the moment\n",
    "## because we haven't yet trained the model\n",
    "print(decode(model.generate(X_test, num_max_token=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd155853-c35a-421a-aaab-045045d8b2ac",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1bbdfb7-5aa8-4c3f-9038-4fcc07051a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## like any other ML model\n",
    "## we need an optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d680a0b1-dc3c-40b9-9073-fa0c0c9611ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss is 4.8435\n",
      "Epoch 1000 Loss is 3.7858\n",
      "Epoch 2000 Loss is 3.2759\n",
      "Epoch 3000 Loss is 2.7724\n",
      "Epoch 4000 Loss is 2.6515\n",
      "Epoch 5000 Loss is 2.5495\n",
      "Epoch 6000 Loss is 2.4680\n",
      "Epoch 7000 Loss is 2.3957\n",
      "Epoch 8000 Loss is 2.5189\n",
      "Epoch 9000 Loss is 2.5125\n",
      "Epoch 10000 Loss is 2.4334\n",
      "Epoch 11000 Loss is 2.3051\n",
      "Epoch 12000 Loss is 2.3299\n",
      "Epoch 13000 Loss is 2.3017\n",
      "Epoch 14000 Loss is 2.4648\n",
      "Epoch 15000 Loss is 2.4087\n",
      "Epoch 16000 Loss is 2.3207\n",
      "Epoch 17000 Loss is 2.4632\n",
      "Epoch 18000 Loss is 2.4763\n",
      "Epoch 19000 Loss is 2.3227\n"
     ]
    }
   ],
   "source": [
    "## the next step is to train the model\n",
    "batch_size = 32\n",
    "epochs = 20000\n",
    "for e in range(epochs):\n",
    "    ## get the X and y\n",
    "    X_train, y_train = get_batch('train')\n",
    "    logits, loss = model(X_train, y_train)\n",
    "    ## zeroing the gradient\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ## and then backpropagation\n",
    "    loss.backward()\n",
    "    ## and then taking a step\n",
    "    optimizer.step()\n",
    "    if e%1000==0:\n",
    "        print(f'Epoch {e} Loss is {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04d9c92b-93ab-45c9-bf33-4ef4907f64c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ster toweabadmoom pngal,\"Bu f t ne I wandns n's. ppedes 3.\n",
      "\n",
      "ey engiousece\n",
      "\n",
      "rothe,\"ckithe besatourskevoly n bleant athy wncleancaros aut\n",
      "\n",
      "\n",
      "\n",
      "Thacecet thout Yon tantt, her averesogond bely stowathe ie, ce angaive. d.\n",
      "\n",
      "anly.\"At at, idins torerowadvo s mamers Zere aplor sinss I'sa, angen at tatheyous he a w t ig\n"
     ]
    }
   ],
   "source": [
    "## now let's check to see what will our model generate after training\n",
    "print(decode(model.generate(X_test, num_max_token=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16766881-e2a2-434c-96f2-f58c6b4ccc36",
   "metadata": {},
   "source": [
    "Still not that good, but certainly better than the first try.\n",
    "\n",
    "Next, we have to start with adding the **self-attention** part to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf632e7-773c-4086-bec3-b25b7e139bb6",
   "metadata": {},
   "source": [
    "### Adding the *self-attention* capability\n",
    "\n",
    "we want any given token in our block to communicate with the tokens that come before it and the simplest way to achieve that will be to get the *average* - bag of words approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b617b730-409f-40f9-838e-bf1f77da8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets suppose we have x\n",
    "## where the dimensions are batch, block = time, and vocab = channel\n",
    "B, T, C = 4, 8, 4\n",
    "x = torch.randn((B, T, C))\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1, :]\n",
    "        xbow[b, t,:] = torch.mean(xprev, 0) ## averaging out all the ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8be2354e-7d5e-4629-8d0f-985b25d9079a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9912, -1.6142,  0.2791,  2.2788],\n",
       "        [ 0.3470,  0.6120,  0.6927,  0.8185],\n",
       "        [ 0.5250,  0.6194, -0.7180, -0.1794],\n",
       "        [-2.2976, -0.4656,  1.4123, -0.0794],\n",
       "        [ 2.6717,  0.1332,  1.1822, -0.3420],\n",
       "        [-0.6930, -0.7369,  1.1706, -0.3187],\n",
       "        [-0.1670, -0.6074,  1.3874, -1.0650],\n",
       "        [ 0.0793, -1.0949, -1.7705,  0.0817]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] ## first batch of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74e46596-a235-462e-9850-b2276b741a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9912, -1.6142,  0.2791,  2.2788],\n",
       "        [-0.3221, -0.5011,  0.4859,  1.5486],\n",
       "        [-0.0397, -0.1276,  0.0846,  0.9726],\n",
       "        [-0.6042, -0.2121,  0.4165,  0.7096],\n",
       "        [ 0.0510, -0.1430,  0.5697,  0.4993],\n",
       "        [-0.0730, -0.2420,  0.6698,  0.3630],\n",
       "        [-0.0865, -0.2942,  0.7723,  0.1590],\n",
       "        [-0.0657, -0.3943,  0.4545,  0.1493]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] ## first batch averged out over the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee018b-7a3c-4e44-b1e8-42af1819d43e",
   "metadata": {},
   "source": [
    "Now, this is very inefficient, and the mathematical way to avoid the loops is to use the *matrix multiplication*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca2747a5-31f0-4e3c-9e02-34c86ec747e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9912, -1.6142,  0.2791,  2.2788],\n",
       "        [-0.3221, -0.5011,  0.4859,  1.5486],\n",
       "        [-0.0397, -0.1276,  0.0846,  0.9726],\n",
       "        [-0.6042, -0.2121,  0.4165,  0.7096],\n",
       "        [ 0.0510, -0.1430,  0.5697,  0.4993],\n",
       "        [-0.0730, -0.2420,  0.6698,  0.3630],\n",
       "        [-0.0865, -0.2942,  0.7723,  0.1590],\n",
       "        [-0.0657, -0.3943,  0.4545,  0.1493]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can create a normalized triangle matrix\n",
    "## and multiply it with the original values\n",
    "## to get the average\n",
    "a = torch.tril(torch.ones((T, T)))\n",
    "a = a / torch.sum(a, 1, keepdim = True)\n",
    "c = a @ x\n",
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd5d8e0e-7dfc-45f0-b2fd-04dcc1c74511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## and now c and xbow are the same\n",
    "torch.allclose(xbow, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2b1726c-7542-451b-b4b2-5141a7f1482e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the other way to write this will be \n",
    "tri = torch.tril(torch.ones((T, T)))\n",
    "w = torch.zeros((T, T))\n",
    "w = w.masked_fill(tri == 0, float('-inf'))\n",
    "w = F.softmax(w, 1)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca6e02-de35-4916-b7aa-cc0203984fce",
   "metadata": {},
   "source": [
    "### Bigram Model + self-attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "503e928a-c178-48ba-b52c-09315c93bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 76])\n",
      "expected loss 4.33\n",
      "claculated loss 4.73\n"
     ]
    }
   ],
   "source": [
    "## adding some new params\n",
    "num_emb = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class BigramLanguageModelV2(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, num_emb=num_emb, block_size=block_size):\n",
    "        super().__init__()\n",
    "        ## we're changing this to be (vocab x emb)\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                           embedding_dim=num_emb)\n",
    "        ## we now want to have another embedding for the positions\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, num_emb)\n",
    "        ## and then a linear layer to give us the logits\n",
    "        self.lin = nn.Linear(num_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, x, target=None):\n",
    "        ## now we'll be incorporating the new layers\n",
    "        B, T = x.shape\n",
    "        token_emb = self.embedding_table(x) ## (B, T, num_emb)\n",
    "        ## and then we can also get the position\n",
    "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device)) ## (T, num_emb)\n",
    "        ## and now our x will be the sum of these two\n",
    "        x = token_emb + pos_emb\n",
    "        ## and we can finally get our logits by the linear layer\n",
    "        logits = self.lin(x) ## (B, T, C)\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, num_max_token):\n",
    "        for _ in range(num_max_token):\n",
    "            ## we want to get the predictions again\n",
    "            logits, loss = self(x)\n",
    "            ## and we only want the last block (Batch, Block, Vocab)\n",
    "            logits = logits[:, -1, :] ## (Batch, Vocab)\n",
    "            ## and then we apply the softmax to get the probabilities\n",
    "            probs = torch.softmax(logits, dim=-1) ## still (Batch, Vocab)\n",
    "            ## and then get a sample from the probablity distribution\n",
    "            next_inx = torch.multinomial(probs,num_samples=1) ## (B, 1)\n",
    "            ## and then append the next index to the x\n",
    "            x = torch.cat((x, next_inx), dim=1) ## (Batch, Block + 1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModelV2(vocab_size = vocab_size, num_emb = num_emb)\n",
    "output, loss = model(X_train, y_train)\n",
    "print(output.shape)\n",
    "## we're expecting the initial entropy to be\n",
    "## -ln(1/vocab_size)\n",
    "print(f'expected loss {-np.log(1/vocab_size):.2f}')\n",
    "print(f'claculated loss {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf40775-fd64-4d1e-8710-287f0ff566dd",
   "metadata": {},
   "source": [
    "### How to add the *self-attention* to our model?\n",
    "\n",
    "Attention is basically a communication mechanism, so in this model, we have the `block_size` nodes, and each of these nodes are communicating the the nodes that come before them, with a weighted vector. This weight depends on what the node has, and what it's looking for/interested in finding.\n",
    "\n",
    "There's no notion of space in this model, there are simply nodes that are out there, unlike the *Convolutional Neural Net*, where the filter goes through the positions/pixels in a space-dependent manner. We have to add the position information ourselves, which is what the `self.pos_embedding_table` does, in our updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "76bd893a-9eef-426f-8b34-2f4a1e35899d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.6163e-03, 9.9438e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.4480e-03, 6.5055e-04, 9.9790e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9243e-02, 3.5913e-02, 7.1229e-02, 8.3362e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.5317e-02, 2.0028e-02, 7.5614e-02, 2.5977e-01, 5.6927e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [8.3936e-03, 9.7194e-03, 2.9207e-03, 1.8211e-03, 4.4495e-03, 9.7270e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [6.3756e-02, 1.0215e-02, 1.2512e-01, 8.6673e-02, 4.2035e-02, 6.4905e-03,\n",
       "         6.6571e-01, 0.0000e+00],\n",
       "        [1.0949e-01, 2.5646e-02, 6.5264e-02, 5.3551e-02, 5.3301e-02, 2.9483e-02,\n",
       "         1.4093e-01, 5.2233e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we'll be introducing the key and query to our model\n",
    "## where the key is the info about the token\n",
    "## and query is what it's looking for\n",
    "## each of these are called a head\n",
    "## and we also need a head_size\n",
    "head_size = 16\n",
    "B, T, C = 4, 8, 32\n",
    "random_x = torch.randn((B, T, C))\n",
    "## and now we need two linear layers\n",
    "## that only have weights\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "## we also need a layer for value\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(random_x) ## (B, T, head_size)\n",
    "q = key(random_x) ## (B, T, head_size)\n",
    "## and now our weights is going to be the product of key and query\n",
    "w = q @ k.transpose(-2, -1) ## (B, T, H) @ (B, H, T) > (B, T, T)\n",
    "## and then the rest is the same as above\n",
    "a = torch.tril(torch.ones((T, T)))\n",
    "w = w.masked_fill(a==0, float('-inf'))\n",
    "w = torch.softmax(w, dim=-1)\n",
    "## and now our weights are not uniform!\n",
    "w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1e58ded6-cf7e-4631-9971-1eb4a575de86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## and before getting the product of the weights and X\n",
    "## we apply the value layer to X, and that's what we aggregate\n",
    "## and the shape now is (B, T, H)\n",
    "v = value(random_x)\n",
    "out = w @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2dd34a8-c57d-4f1f-b247-21357af19b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2152, 0.7848, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1440, 0.1179, 0.7380, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2055, 0.1813, 0.2152, 0.3980, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1742, 0.1251, 0.1744, 0.2374, 0.2889, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1312, 0.1361, 0.1008, 0.0895, 0.1119, 0.4305, 0.0000, 0.0000],\n",
       "        [0.1397, 0.0884, 0.1653, 0.1508, 0.1259, 0.0789, 0.2511, 0.0000],\n",
       "        [0.1332, 0.0926, 0.1170, 0.1114, 0.1112, 0.0959, 0.1418, 0.1968]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we also need to scale the Q @ K product\n",
    "## so that the weights are not too concenterated \n",
    "## especially at the beginning, because we don't want our nodes\n",
    "## to only learn from one other node, and we want the weights to be diffused\n",
    "## and the way to normalize it is to devide them by sqrt of head size\n",
    "w = q @ k.transpose(-2, -1) * head_size ** -.5\n",
    "a = torch.tril(torch.ones((T, T)))\n",
    "## the reason we have this is so that the nodes won't\n",
    "## communicate with the nodes that come after them\n",
    "w = w.masked_fill(a == 0, float('-inf'))\n",
    "w = F.softmax(w, dim =-1)\n",
    "v = value(random_x)\n",
    "out = w @ v\n",
    "w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffdaa1-6272-4613-85ea-662d307955c5",
   "metadata": {},
   "source": [
    "### The actual head class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "74b64d0e-9f45-4743-a93d-38c996847575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## the head class will use the self-attention concepts\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size=head_size, num_embd = num_emb):\n",
    "        super().__init__()\n",
    "        ## we have a linear layer for keys\n",
    "        ## one for queries and another for values\n",
    "        self.key = nn.Linear(num_embd, head_size, bias=False)\n",
    "        self.quey = nn.Linear(num_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embd, head_size, bias=False)\n",
    "        ## we also have the triangle that we use for masking\n",
    "        self.register_buffer(name='tril',tensor=torch.tril(torch.ones((block_size, block_size))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape ## C = head_size\n",
    "        k = self.key(x) ## (B, T, num_emb)\n",
    "        q = self.quey(x) ## (B, T, num_emb)\n",
    "        weight = q @ k.transpose(-2, -1) * C ** -0.5 ## (B, T, T)\n",
    "        weight = weight.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        v = self.value(x) ## (B, T, C)\n",
    "        out = weight @ v ## (B, T, C)\n",
    "        return out\n",
    "    \n",
    "## in order to improve our model furthur\n",
    "## we can have multiple heads\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size) for _ in range(num_heads)])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9a01ebdc-9ada-44e7-97fc-6f1edd5510f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 76])\n",
      "expected loss 4.33\n",
      "claculated loss 4.34\n"
     ]
    }
   ],
   "source": [
    "## updating the model\n",
    "class BigramLanguageModelV3(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, num_emb=num_emb, block_size=block_size):\n",
    "        super().__init__()\n",
    "        ## we're changing this to be (vocab x emb)\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                           embedding_dim=num_emb)\n",
    "        ## we now want to have another embedding for the positions\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, num_emb)\n",
    "        self.head = Head(num_emb, num_emb)\n",
    "        ## and then a linear layer to give us the logits\n",
    "        self.lin_head = nn.Linear(num_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, x, target=None):\n",
    "        ## now we'll be incorporating the new layers\n",
    "        B, T = x.shape\n",
    "        token_emb = self.embedding_table(x) ## (B, T, num_emb)\n",
    "        ## and then we can also get the position\n",
    "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device)) ## (T, num_emb)\n",
    "        ## and now our x will be the sum of these two\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.head(x)\n",
    "        ## and we can finally get our logits by the linear layer\n",
    "        logits = self.lin_head(x) ## (B, T, C)\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, num_max_token):\n",
    "        for _ in range(num_max_token):\n",
    "            ## we want to get the predictions again\n",
    "            logits, loss = self(x)\n",
    "            ## and we only want the last block (Batch, Block, Vocab)\n",
    "            logits = logits[:, -1, :] ## (Batch, Vocab)\n",
    "            ## and then we apply the softmax to get the probabilities\n",
    "            probs = torch.softmax(logits, dim=-1) ## still (Batch, Vocab)\n",
    "            ## and then get a sample from the probablity distribution\n",
    "            next_inx = torch.multinomial(probs,num_samples=1) ## (B, 1)\n",
    "            ## and then append the next index to the x\n",
    "            x = torch.cat((x, next_inx), dim=1) ## (Batch, Block + 1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModelV3(vocab_size = vocab_size, num_emb = num_emb)\n",
    "output, loss = model(X_train, y_train)\n",
    "print(output.shape)\n",
    "## we're expecting the initial entropy to be\n",
    "## -ln(1/vocab_size)\n",
    "print(f'expected loss {-np.log(1/vocab_size):.2f}')\n",
    "print(f'claculated loss {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad9e10-9543-4394-adaa-1f9609168534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the new model and checking the loss \n",
    "## to turn off the gradient calculation\n",
    "@torch.inference_mode()\n",
    "def estimate_loss(evaluation_interval):\n",
    "    out = {}\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(evaluation_interval)        \n",
    "        for i in range(evaluation_interval):\n",
    "            xs, ys = get_batch(split)\n",
    "            model.eval()\n",
    "            _, loss = model(xs, ys)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()   \n",
    "    model.train()     \n",
    "    return out\n",
    "\n",
    "epochs = 5000\n",
    "evaluation_interval = 500\n",
    "for e in range(epochs):\n",
    "    xs, ys = get_batch('train')\n",
    "    logits, loss = model(xs, ys)\n",
    "    ## zero out the gradient\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    ## and then backpropagation\n",
    "    loss.backward()\n",
    "    ## and then the optimizer step\n",
    "    optimizer.step()\n",
    "    if e%evaluation_interval==0:\n",
    "        result = estimate_loss(evaluation_interval)\n",
    "        print(f\"Epoch {e} average train loss is {result['train']:.2f} | average test loss is {result['test']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
