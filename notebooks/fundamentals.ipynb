{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9245324f-0c45-436d-8113-5f8638226bb2",
   "metadata": {},
   "source": [
    "### LLM Fundamentals\n",
    "\n",
    "In this notebook we will go through the fundamentals of the **LLM**.\n",
    "\n",
    "The steps are as follows:\n",
    "- Load the data\n",
    "- Encode the data\n",
    "- Converting our text to tensors\n",
    "- Train/test split\n",
    "- Staring with Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c955044e-1ba4-4123-be75-add835cf19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first the imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fec8c-5aeb-42b0-9866-877d111165d6",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4d68be-4c84-47c3-b896-b106648091c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = Path(DATA_PATH)\n",
    "FILE_NAME = 'wizardOfOz.txt'\n",
    "FILE_PATH = PATH / FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60db8bc7-ab79-4e4e-ba55-08b437d8c14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿ Dorothy and the Wizard in Oz\n",
      "\n",
      "\n",
      "  A Faithful Record of Their Amazing Adventures\n",
      "    in an Undergrou\n"
     ]
    }
   ],
   "source": [
    "## opening the file\n",
    "with open(FILE_PATH, 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "## checking the first 100 char\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb01b39-49f2-4a28-8e88-0f62a2f10c80",
   "metadata": {},
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5e9358-2432-4c91-b756-cddf520b73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "## first we want to create a set of char\n",
    "chars = sorted(set(text))\n",
    "## checking how many distinct char are in the text\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "## and then assign a number to each char\n",
    "int_to_string = {i:l for i, l in enumerate(chars)}\n",
    "string_to_int = {l:i for i, l in enumerate(chars)}\n",
    "## and then move on to creating our encoding and decoding functions\n",
    "encode = lambda l:[string_to_int[x] for x in l]\n",
    "decode = lambda i:''.join([int_to_string[x] for x in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ba0b36-3075-4cc8-8638-9a7d8f7664e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 63, 66, 63, 68, 56, 73]\n",
      "Dorothy\n"
     ]
    }
   ],
   "source": [
    "## we can test our encoder and decoder now\n",
    "print(encode('Dorothy'))\n",
    "print(decode(encode('Dorothy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f55bf-9885-4359-9878-2f7ee6a1c922",
   "metadata": {},
   "source": [
    "### Converting our text to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a241ad82-eb62-4792-965b-c69df279837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([230550]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([75,  1, 27, 63, 66, 63, 68, 56, 73,  1, 49, 62, 52,  1, 68, 56, 53,  1,\n",
       "        46, 57, 74, 49, 66, 52,  1, 57, 62,  1, 38, 74,  0,  0,  0,  1,  1, 24,\n",
       "         1, 29, 49, 57, 68, 56, 54, 69, 60,  1, 41, 53, 51, 63, 66, 52,  1, 63,\n",
       "        54,  1, 43, 56, 53, 57, 66,  1, 24, 61, 49, 74, 57, 62, 55,  1, 24, 52,\n",
       "        70, 53, 62, 68, 69, 66, 53, 67,  0,  1,  1,  1,  1, 57, 62,  1, 49, 62,\n",
       "         1, 44, 62, 52, 53, 66, 55, 66, 63, 69])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tensor = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(text_tensor.size(), type(text_tensor))\n",
    "text_tensor[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50ecc10d-c1dc-452b-984f-5e8c6a238651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([184440]), torch.Size([46110]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we're simply splitting the data into train and test sets\n",
    "train_data, test_data = np.split(text_tensor, [int(.8*len(text_tensor))])\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f6ccaa-98e0-4e8a-8592-92c633bf5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the context is tensor([75]) the target will be 1\n",
      "When the context is tensor([75,  1]) the target will be 27\n",
      "When the context is tensor([75,  1, 27]) the target will be 63\n",
      "When the context is tensor([75,  1, 27, 63]) the target will be 66\n",
      "When the context is tensor([75,  1, 27, 63, 66]) the target will be 63\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63]) the target will be 68\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63, 68]) the target will be 56\n",
      "When the context is tensor([75,  1, 27, 63, 66, 63, 68, 56]) the target will be 73\n"
     ]
    }
   ],
   "source": [
    "## next we have to define a block size for our model\n",
    "block_size = 8\n",
    "## this means, the model will look at 8 sequences\n",
    "## at each round of training\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f'When the context is {context} the target will be {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90440876-1737-4dbd-93b5-bf6ad6829ca1",
   "metadata": {},
   "source": [
    "The reason we're looping through the `block_size` range, is to have our model get used to seeing anything from `1` to the `block_size` length of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "525eac3b-c93c-4c28-8bff-9de027f65fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "## we also need to break our data into batches for faster computations\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    random_inx = torch.randint(high=len(data)-block_size, size = (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in random_inx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_inx])\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = get_batch('train')\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3d88b3a-8b1d-42dc-ba4a-c7ffc23bdddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: When context is tensor([66]) the target is 67\n",
      "Batch 0: When context is tensor([66, 67]) the target is 53\n",
      "Batch 0: When context is tensor([66, 67, 53]) the target is 10\n",
      "Batch 0: When context is tensor([66, 67, 53, 10]) the target is 3\n",
      "Batch 0: When context is tensor([66, 67, 53, 10,  3]) the target is 0\n",
      "Batch 0: When context is tensor([66, 67, 53, 10,  3,  0]) the target is 0\n",
      "Batch 0: When context is tensor([66, 67, 53, 10,  3,  0,  0]) the target is 3\n",
      "Batch 0: When context is tensor([66, 67, 53, 10,  3,  0,  0,  3]) the target is 24\n",
      "Batch 1: When context is tensor([67]) the target is 1\n",
      "Batch 1: When context is tensor([67,  1]) the target is 44\n",
      "Batch 1: When context is tensor([67,  1, 44]) the target is 62\n",
      "Batch 1: When context is tensor([67,  1, 44, 62]) the target is 51\n",
      "Batch 1: When context is tensor([67,  1, 44, 62, 51]) the target is 60\n",
      "Batch 1: When context is tensor([67,  1, 44, 62, 51, 60]) the target is 53\n",
      "Batch 1: When context is tensor([67,  1, 44, 62, 51, 60, 53]) the target is 1\n",
      "Batch 1: When context is tensor([67,  1, 44, 62, 51, 60, 53,  1]) the target is 31\n",
      "Batch 2: When context is tensor([53]) the target is 1\n",
      "Batch 2: When context is tensor([53,  1]) the target is 46\n",
      "Batch 2: When context is tensor([53,  1, 46]) the target is 57\n",
      "Batch 2: When context is tensor([53,  1, 46, 57]) the target is 74\n",
      "Batch 2: When context is tensor([53,  1, 46, 57, 74]) the target is 49\n",
      "Batch 2: When context is tensor([53,  1, 46, 57, 74, 49]) the target is 66\n",
      "Batch 2: When context is tensor([53,  1, 46, 57, 74, 49, 66]) the target is 52\n",
      "Batch 2: When context is tensor([53,  1, 46, 57, 74, 49, 66, 52]) the target is 1\n",
      "Batch 3: When context is tensor([53]) the target is 64\n",
      "Batch 3: When context is tensor([53, 64]) the target is 1\n",
      "Batch 3: When context is tensor([53, 64,  1]) the target is 68\n",
      "Batch 3: When context is tensor([53, 64,  1, 68]) the target is 56\n",
      "Batch 3: When context is tensor([53, 64,  1, 68, 56]) the target is 57\n",
      "Batch 3: When context is tensor([53, 64,  1, 68, 56, 57]) the target is 67\n",
      "Batch 3: When context is tensor([53, 64,  1, 68, 56, 57, 67]) the target is 1\n",
      "Batch 3: When context is tensor([53, 64,  1, 68, 56, 57, 67,  1]) the target is 46\n"
     ]
    }
   ],
   "source": [
    "## and we can loop through the batches in our set\n",
    "for b in range(batch_size):\n",
    "    for i in range(block_size):\n",
    "        context = X_train[b,:i+1]\n",
    "        target = y_train[b, i]\n",
    "        print(f'Batch {b}: When context is {context} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0e1ac-24dd-4e29-a84c-7a63b2c35267",
   "metadata": {},
   "source": [
    "### The Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8e5cb-57f6-4565-98eb-1520e60f7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will be inheriting from the nn.Module\n",
    "## and then use the embedding from nn to build our class\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                           embedding_dim=vocab_size)\n",
    "    def forward(self, x, target):\n",
    "        return self.embedding_table(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
